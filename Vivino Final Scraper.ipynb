{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7356d9b9",
   "metadata": {},
   "source": [
    "# 1. Load URL to Python using Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc289789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant packages used in this script\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import csv\n",
    "from time import sleep\n",
    "\n",
    "#load chrome webdriver \n",
    "driver = webdriver.Chrome() #,chrome_options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77d3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver.get('https://www.vivino.com')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ba51d",
   "metadata": {},
   "source": [
    "### Why selenium? \n",
    "As you can see we are using selenium to scrape our data. Selenium lets us scrape the page as if we are users: this withholds us from errors and blocks from the website. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3b890",
   "metadata": {},
   "source": [
    "# 2. Set up correct settings for chromedriver window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec66d25",
   "metadata": {},
   "source": [
    "### To be able to scrape the data in our interest, it is important to set up the correct chromedriver window. Via selenium we let the computer navigate to all the correct settings. \n",
    "\n",
    "\n",
    "<b>Maximize Chromedriver window:</b> We maximize the chromedriver window to be able to scrape all the correct data. Besides, it makes our scraper more universally usable, since now it is sure that for all users of our scraper the chromedriver window is maximized and thus the scraper will scrape the right code. \n",
    "\n",
    "<b>Click cookie consent button:</b> To make our code more neat, we let the computer click the cookie consent button. Besides that it makes the scrolling process more visible. \n",
    "\n",
    "<b>Navigate to catelogue page:</b>\n",
    "Since we want to scrape the wines from Vivino, it is an important step to let the computer navigate to the wines page. \n",
    "\n",
    "<b>Navigate to all red wines:</b>\n",
    "Our sample size only contains red wines. That is why we let the computer filter for red wines only. The default buttons for the filter are \"Red wines\" and \"White wines\". Therefore, we let the computer click on the \"White wines\" filter, to actually de-filter for \"White wines\". Only the red wines are left now. \n",
    "\n",
    "<b>Click sort button:</b>\n",
    "The default value of the sort by function is sort on highest rating. Since this would bias our research we adjust this to sort by price from low to high. To be able to click an option, first the sort button has to be clicked on. \n",
    "\n",
    "<b>Sort from low to high price:</b>\n",
    "Now the options of the sort button are visible, we click price: low to high, as said before. \n",
    "\n",
    "<b>Scroll a bit lower to be able to see price range and average rating filter</b>\n",
    "To be able to check whether the price range and the average rating filter will be set correctly we scroll a bit downwards\n",
    "\n",
    "<b>Set price range from 0 to 500+:</b>\n",
    "The default value of the price range is €7 - €30. In the scope of our research motivation we change this to the full price range. This is done by the \"drag and drop by offset\" function within the ActionChains module. \n",
    "\n",
    "<b>Show wine of all ratings:</b>\n",
    "The default value of the average rating for which wines are shown, is all wines with rating 3.8 or higher. In the interest of our research motivation it is an important step to change this filter. We let the computer click on \"Show all ratings\". \n",
    "\n",
    "<b>Sroll down to load regions button:</b>\n",
    "Since we will scrape the first 500 wines shown for all different regions, it is important to load the regions button. For this, we need to scroll down to the place these buttons are visible, since only then the buttons will load and consequently can be scraped. \n",
    "\n",
    "#### Final Window: \n",
    "Ultimately, we the page we are going to scrape is a maximized window that contains the Wines page of Vivino.com, Filtered on: Red Wines, Full Price Range, All Average Ratings and sorted by price from low to high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfec9da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_window(): \n",
    "    #maximize chromedriver window \n",
    "    driver.maximize_window()\n",
    "    \n",
    "    #sleep 5 seconds to make sure page is loaded\n",
    "    sleep(5)\n",
    "    #click cookie consent button\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"cookie-notice-container\"]/div/button/span/span')\n",
    "    element.click()\n",
    "    sleep(2)\n",
    "    #navigate to catalogue page\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"navigation-container\"]/div/div[2]/div/div[1]/div/a/span[2]')\n",
    "    element.click()\n",
    "    sleep(2)\n",
    "    #navigate to all red wines\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"explore-page-app\"]/div/div/div[2]/div[1]/div/div[1]/div[2]/label[2]/div')\n",
    "    element.click()\n",
    "    sleep(2)\n",
    "    #click \"sort\" button to be able to change sort by\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"explore-page-app\"]/div/div/div[1]/div[2]/div')\n",
    "    element.click()\n",
    "    sleep(2)\n",
    "    #click \"price: low to high\" within sort button\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"menu-\"]/div[3]/ul/li[4]')\n",
    "    element.click()\n",
    "    sleep(2)\n",
    "    #scroll a bit lower to be able to see price range and average rating filter\n",
    "    driver.execute_script('window.scrollTo(0, ' + str(500) + ')')\n",
    "    \n",
    "    #set price range from €0 to €500+\n",
    "    elem1 = driver.find_element(By.XPATH,'//*[@id=\"explore-page-app\"]/div/div/div[2]/div[1]/div/div[2]/div[2]/div[2]/div/div[4]')\n",
    "    elem2 = driver.find_element(By.XPATH,'//*[@id=\"explore-page-app\"]/div/div/div[2]/div[1]/div/div[2]/div[2]/div[2]/div/div[5]')\n",
    "    ActionChains(driver).drag_and_drop_by_offset(elem1, -50,0).perform()\n",
    "    ActionChains(driver).drag_and_drop_by_offset(elem2, 200,0).perform()\n",
    "    sleep(2)\n",
    "    #show wines of all average ratings\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"1\"]')\n",
    "    element.click()\n",
    "    sleep(2)\n",
    "    #scroll down to see regions filter, and wait 15 seconds to be sure all regions are loaded\n",
    "    driver.execute_script('window.scrollTo(0, ' + str(1000) + ')')\n",
    "    sleep(15)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa9182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call prepare_window function\n",
    "prepare_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aa88fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "res = driver.page_source.encode('utf-8')\n",
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e4ebfa",
   "metadata": {},
   "source": [
    "# 3. Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c65c48",
   "metadata": {},
   "source": [
    "#### Get_the_url function\n",
    "We defined a get_the_url_function. The input value of this function is one of the Regions we want to scrape. The computer then tries to find a filter button linked to this Region and clicks it. By doing this, the page will be filtered for that Region and can then be scraped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9086e822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define get_the_url function which automatically clicks the Region to scrape in the filter on the website\n",
    "def get_the_url(Region):\n",
    "    element= driver.find_element_by_xpath(\"//span[text()='\"+ Region + \"']\")\n",
    "    element.click()\n",
    "    url = driver.current_url\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3986b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a scroll range high enough to scrape 500 wines\n",
    "the_range = int(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8c5e3",
   "metadata": {},
   "source": [
    "#### Scroll_page function:\n",
    "We defined a scroll_page function. The input value of this function is also one of the Regions. Then, the get_the_url function is put within the scroll_page function. So for example if we say scroll_page(\"Bordeaux), the scroll_page function will first navigate to the correct url (the wines that are filtered by Region \"Bordeaux\"). \n",
    "\n",
    "Then, a variable <b>\"numbers\"</b> is created, which tells the total amount of wines that is shown on the page. This variable will later on be used to stop the scraper when all wines are scraped. \n",
    "\n",
    "To be able to later on filter out duplicate wines, we create a variable <b>\"scraping_round\"</b>. For every time the full page is being scraped, the variable scraping_round will go up by 1. \n",
    "\n",
    "Then, a for-loop is created. This first for-loop scrolls through the website. As seen at the bottom of the function, scroll_range goes up by 1000 every time it goes through the loop. \n",
    "\n",
    "To <b>make our scraper more efficient</b>, we only scrape the page when there are more wines visible than in the previous scroll loop. If this is the case, the computer will perform the next for-loop: the scraping. The for loop scrapes all wines visible in the current window. To be precisely, it scrapes 4 attributes of all the wines: the name, the price, the number of reviews and the average rating. These are all found by searching via the find_all function within BeautifulSoup. \n",
    "\n",
    "The attributes that are scraped from the wines are then <b>transformed</b> for our users to better be able to work with the output data. \n",
    "\n",
    "Then, the transformed attributes per wine are written to a <b>csv</b> file that holds the name of the Region. \n",
    "\n",
    "To make sure our scroller does not unnescessarily scroll the amount of times put under the high number of the_range, we create an <b>if-statement</b> which stops the scroller when the total amount of wines scraped per Region are a given number (in this case 500), or when the total amount of wines scraped is already the total amount of wines that are available for scraping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "919645d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_page(Region):    \n",
    "    \n",
    "    #call get_url function to get the correct url linked to the Region that is going to be scraped\n",
    "    get_the_url(Region)      \n",
    "    \n",
    "    sleep(1)\n",
    "    \n",
    "    res = driver.page_source.encode('utf-8')\n",
    "    soup = BeautifulSoup(res, \"html.parser\")\n",
    "    scroll_range = 0\n",
    "    \n",
    "    sleep (5)\n",
    "        \n",
    "    #create variable numbers which finds the total amount of wines diplayed at the page that is being scraped\n",
    "    all_wines = driver.find_element_by_xpath('//*[@id=\"explore-page-app\"]/div/div/h2').text\n",
    "    numbers = [int(word) for word in all_wines.split() if word.isdigit()][0]\n",
    "    numbers = int(numbers)\n",
    "    \n",
    "    #create scraping_round variable that counts the amount of times all the wines are being scraped \n",
    "    scraping_round = 1\n",
    "        \n",
    "    num_wines_view_2 = 0\n",
    "    #create a scroller that scrolls the_range amount of times.     \n",
    "    for _ in range(the_range):\n",
    "        res = driver.page_source.encode('utf-8')\n",
    "        soup = BeautifulSoup(res, \"html.parser\")\n",
    "        \n",
    "        \n",
    "        # total number of wines in current view\n",
    "        num_wines_view_1 = int(len(soup.find_all(attrs={\"data-testid\" : \"wineCard\"})))\n",
    "        \n",
    "        #create attributes the_name_id, the_price_id, the_reviews_id and the_rating id\n",
    "        if num_wines_view_1 > num_wines_view_2:\n",
    "            for counter in range(num_wines_view_1):\n",
    "                the_name_id = soup.find_all(attrs={\"data-testid\": \"wineCard\"})[counter].find_all(attrs={\"class\": \"wineInfoVintage__vintage--VvWlU wineInfoVintage__truncate--3QAtw\"})[0].text\n",
    "                the_price_id = soup.find_all(attrs={\"data-testid\": \"wineCard\"})[counter].find_all(True, {\"class\": [\"addToCartButton__price--qJdh4\" , \"addToCart__subText--1pvFt addToCart__ppcPrice--ydrd5\", \"addToCart__subText--1pvFt addToCart__soldOut--1dP2Z\"]})[0].text\n",
    "                the_reviews_id =  soup.find_all(attrs={\"data-testid\": \"wineCard\"})[counter].find_all(attrs={\"class\": \"vivinoRating__caption--3tZeS\"})[0].text\n",
    "                the_rating_id = soup.find_all(attrs={\"data-testid\": \"wineCard\"})[counter].find_all(attrs={\"class\": \"vivinoRating__averageValue--3Navj\"})[0].text    \n",
    "            \n",
    "                #reshape the_price_id variable that is put into the csv file to make it more usable in further analysis\n",
    "                try:\n",
    "                    the_price_id = the_price_id.replace(\"€\", \"\")\n",
    "                    the_price_id = the_price_id.replace(\".\", \"\")\n",
    "                    the_price_id = the_price_id.replace(\",\", \".\")\n",
    "                    the_price_id = the_price_id.split(\" \")\n",
    "                    the_price_id = float(the_price_id[-1])\n",
    "                except: \n",
    "                    the_price_id = \"Sold out, no price available\"\n",
    "                \n",
    "                #reshape the_reviews_id variable that is put into the csv file to make it more usable in further analysis\n",
    "                the_reviews_id = the_reviews_id.split(\" \")\n",
    "                the_reviews_id = int(the_reviews_id[0])\n",
    "                \n",
    "                \n",
    "                #name the file that is going to be created \n",
    "                filename = Region\n",
    "                fullpath = str(filename) + \".csv\"\n",
    "                #write variables per wine into csv file \n",
    "                with open(fullpath, mode='a', newline='', encoding='utf-8') as csv_file:\n",
    "                    writer = csv.writer(csv_file)\n",
    "                    writer.writerow([scraping_round, Region, the_name_id, the_price_id, the_reviews_id, the_rating_id])\n",
    "                    \n",
    "            #scraping_round counter +1        \n",
    "            scraping_round += 1\n",
    "        \n",
    "        #the scroller scrolls in range +1000 in every loop \n",
    "        scroll_range += 1000\n",
    "        driver.execute_script('window.scrollTo(0, ' + str(scroll_range) + ')')\n",
    "        \n",
    "        #update total number of wines in current view to compare with number of wines in view before scrolling\n",
    "        num_wines_view_2 = int(len(soup.find_all(attrs={\"data-testid\" : \"wineCard\"})))\n",
    "        \n",
    "        #break loop if 500 wines are scraped\n",
    "        if num_wines_view_2 >= int(500): \n",
    "            break\n",
    "        #break loop if total number of wines are scraped\n",
    "        if num_wines_view_2 == numbers:\n",
    "            break\n",
    "    \n",
    "        # pause for 5 seconds\n",
    "        sleep(1)   \n",
    "    get_the_url(Region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60734225",
   "metadata": {},
   "source": [
    "#### Filter_file Function\n",
    "\n",
    "Now, the output file we created in the scroll_page function contains a lot of duplicate numbers. This is because the scraper we created scrapes the full page every time. This is where the variable \"scraping_round\" comes to use: we will filter the csv file as to only leave the wines of the last scraping round, a.k.a. all the wines visible in the last (furthest scrolled) window. The filtered files are saved as a new file: Region + \"filtered.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e885b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_file(Region):\n",
    "    \n",
    "    #define filenames\n",
    "    fullpath = Region + \".csv\"\n",
    "    filtered_file = Region + ' filtered.csv'\n",
    "    \n",
    "    #assign a variable to the maximum (a.k.a. last) scraping round, stated in the first column of the csv file    \n",
    "    answer = int(max(int(column[0].replace(',', '')) for column in csv.reader(open(fullpath,'r'))))\n",
    "    \n",
    "    #write all wines that are scraped in the last scraping round to a csv \"filtered_file\".\n",
    "    with open(fullpath,'r') as fin:\n",
    "        with open(filtered_file,'w', newline='') as fout:\n",
    "            header = [\"Scraping Round\", \"Region\", \"Name\", \"Price\", \"Number of Ratings\", \"Average Rating\"]\n",
    "            csv.writer(fout).writerow(header)\n",
    "            for row in csv.reader(fin):\n",
    "                if int(row[0]) == int(answer):\n",
    "                    csv.writer(fout).writerow(row)\n",
    "                    \n",
    "              \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99665839",
   "metadata": {},
   "source": [
    "#### Delete_files Function\n",
    "Now we have a unfiltered and a filtered file per region, we will delete all the unfiltered files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14aa0980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all files that contain the duplicated data, and keep al the filtered files in the folder\n",
    "def delete_files(Region):\n",
    "    import os\n",
    "    fullpath = Region + \".csv\"\n",
    "    os.remove(fullpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccaa6f2",
   "metadata": {},
   "source": [
    "# 4. Call Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9223dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of all regions\n",
    "total_regions_list = []\n",
    "for counter in range(0,6):\n",
    "    total_regions_list.append(soup.find_all(attrs={\"class\": \"filterPills__items--_grOA\"})[1].find_all(attrs={\"class\": \"pill__text--24qI1\"})[counter].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a68a902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bordeaux', 'Bourgogne', 'Napa Valley', 'Piemonte', 'Rhone Valley', 'Toscana']\n"
     ]
    }
   ],
   "source": [
    "#check whether Regions list is correct\n",
    "print(total_regions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the scroller and filter function for all Regions\n",
    "for Region in total_regions_list: \n",
    "    scroll_page(Region)\n",
    "    filter_file(Region)\n",
    "    delete_files(Region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bcdc8b",
   "metadata": {},
   "source": [
    "# 5. Create One Final output File "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80abcb9",
   "metadata": {},
   "source": [
    "#### Merge_files Function \n",
    "Now we are left with the filtered files per Region. We now merge all these files together to get one big final output files, containing the wines of all scraped Regions together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b438129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to merge the filtered files to get a complete dataset\n",
    "def merge_files():\n",
    "    import os\n",
    "    import glob\n",
    "    extension = 'csv'\n",
    "    all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "    #combine all files in the list\n",
    "    combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames])\n",
    "    #export to csv\n",
    "    combined_csv.to_csv( \"All_regions_data.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0214957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call merge_files function\n",
    "merge_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b3605",
   "metadata": {},
   "source": [
    "#### Delete_filtered_files Function \n",
    "Now we obtained the big final output files, all the loose filtered files per Region can be deleted. This makes the output folder more neat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa3a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_filtered_files(Region):\n",
    "    fullpath = Region + \" filtered.csv\"\n",
    "    os.remove(fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75912d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call delete_filtered_files function for all regions\n",
    "for Region in total_regions_list: \n",
    "    delete_filtered_files(Region)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
